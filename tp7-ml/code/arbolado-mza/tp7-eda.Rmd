---
title: "tp7-eda.md"
output: html_notebook
---

```{r}
install.packages("dplyr")
install.packages("ggplot2")
install.packages("caret")
install.packages("randomForest")
```

```{r}
set.seed(123)
library(dplyr)
library(ggplot2)
library(caret)
library(rpart)
library(utils)
library(randomForest)
```

```{r}
arbolado.mza.dataset <- read.csv("~/workspace/arbolado-mza/arbolado-mza-dataset.csv/arbolado-mza-dataset.csv")
arbolado.mza.dataset <- as.data.frame(arbolado.mza.dataset)
```

# 1.a

```{r}
indices <- createDataPartition(arbolado.mza.dataset$inclinacion_peligrosa, p = 0.8, list = FALSE)

training_data <- arbolado.mza.dataset[indices, ]
testing_data <- arbolado.mza.dataset[-indices, ]
write.csv(testing_data, "arbolado-mendoza-validation.csv")
write.csv(training_data, "arbolado-mendoza.csv")
testing_data
```
# 2.a

Parecen haber muchos (22695 vs 2835) mas ejemplares con inclinacion no peligrosa

```{r}
result <- training_data %>%
  dplyr::group_by(inclinacion_peligrosa) %>%
  dplyr::summarize(Count=n())
x_val <- barplot(result$Count, names.arg=c("No Peligrosa", "Peligrosa"))
# Add text labels on top of the bars
text(x = x_val, y = result$Count, labels = result$Count, pos = 1, cex = 1)
```
# 2.b

La seccion mas peligrosa parece ser la seccion 3.

Cabe destacar que no tenemos informacion para las secciones 9 y 10 y muy poca info de las secciones 7 y 8

```{r}
result <- training_data %>%
  dplyr::group_by(seccion) %>%
  dplyr::summarize(peligrosa=sum(inclinacion_peligrosa), total=n())
ggplot(data=result, aes(x=seccion, fill="a")) +
  geom_bar(aes(y=total), stat="identity", position = "dodge", alpha=1, width=0.7, fill="darkgreen") +
  labs(title="Por seccion", x="Seccion", y="Total") +
  geom_bar(aes(y=peligrosa), data = result, stat = "identity", position="dodge", alpha = 1, width=0.7, fill="red") +
  scale_x_continuous(breaks = result$seccion, expand = c(0, 0))+
  scale_fill_manual(values = c("Total" = "darkgreen", "Peligrosas" = "red")) +
  guides(fill = guide_legend(title = "Por seccion")) +
  theme_minimal()

result <- training_data %>%
  dplyr::group_by(seccion) %>%
  dplyr::summarize(peligrosa=sum(inclinacion_peligrosa)/ifelse(n() == 0, sum(inclinacion_peligrosa), n()), total=1)

ggplot(data=result, aes(x=seccion, fill="a")) +
  geom_bar(aes(y=total), stat="identity", position = "dodge", alpha=1, width=0.7, fill="darkgreen") +
  labs(title="Por seccion", x="Seccion", y="Peligrosidad") +
  geom_bar(aes(y=peligrosa), data = result, stat = "identity", position="dodge", alpha = 1, width=0.7, fill="red") +
  scale_x_continuous(breaks = result$seccion, expand = c(0, 0))+
  scale_fill_manual(values = c("Total" = "darkgreen", "Peligrosas" = "red")) +
  guides(fill = guide_legend(title = "Por seccion Normalizada")) +
  theme_minimal()
```
# 3.c

Al parecer tenemos muchisima informacion de Moreras, en comparacion de otras especies.

Aun asi, los algarrobos parecen presentar una inclinacion peligrosa con mayor frequencia.

Dicho esto, solo tenemos 5 ejemplares de algarrobo en nuestro dataset de entrenamiento.

```{r}
result <- training_data %>%
  dplyr::group_by(especie) %>%
  dplyr::summarize(peligrosa=sum(inclinacion_peligrosa), total=n())

result$especie = reorder(result$especie, -result$total)
ggplot(data=result, aes(x=especie, fill="a")) +
  geom_bar(aes(y=total), stat="identity", position = "dodge", alpha=1, width=0.7, fill="darkgreen") +
  labs(title="Por especie", x="Especie", y="Total") +
  geom_bar(aes(y=peligrosa), data = result, stat = "identity", position="dodge", alpha = 1, width=0.7, fill="red") +
  scale_x_discrete(breaks = result$especie, expand = c(0, 0))+
  scale_fill_manual(values = c("Total" = "darkgreen", "Peligrosas" = "red")) +
  guides(fill = guide_legend(title = "Por especie")) +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))


result <- training_data %>%
  dplyr::group_by(especie) %>%
  dplyr::summarize(peligrosa=sum(inclinacion_peligrosa)/ifelse(n() == 0, sum(inclinacion_peligrosa), n()), total=1)

result$especie = reorder(result$especie, -result$peligrosa)
ggplot(data=result, aes(x=especie, fill="a")) +
  geom_bar(aes(y=total), stat="identity", position = "dodge", alpha=1, width=0.7, fill="darkgreen") +
  labs(title="Por especie", x="Especie", y="Total") +
  geom_bar(aes(y=peligrosa), data = result, stat = "identity", position="dodge", alpha = 1, width=0.7, fill="red") +
  scale_x_discrete(breaks = result$especie, expand = c(0, 0))+
  scale_fill_manual(values = c("Total" = "darkgreen", "Peligrosas" = "red")) +
  guides(fill = guide_legend(title = "Por especie")) +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
training_data %>% filter(especie == "Algarrobo")
```

# 3.b

```{r}
hist_data <- hist(training_data$circ_tronco_cm, breaks=seq(min(training_data$circ_tronco_cm) - 2, max(training_data$circ_tronco_cm) + 10, by=10), main="Histograma de Circ Tronco", xlab="Circ Tronco", ylab="Frecuencia", col="lightblue", border="black")
## plot(hist_data)
```
# 3.c

No es una pregunta.

No tiene mucho sentido hacer esto, ya que solo nos da la suma total por clase, que ya contamos antes.

```{r}
hist_data <- hist(training_data$inclinacion_peligrosa, breaks=seq(min(training_data$inclinacion_peligrosa) - 0.5, max(training_data$inclinacion_peligrosa) + 0.5, by=1), main="Histograma de Inclinacion peligrosa", xlab="Inclinacion peligrosa", ylab="Frecuencia", col="lightblue", border="black")
## plot(hist_data)

```

```{r}
result <- training_data %>%
  mutate(
    circ_tronco_cm_cat = case_when(
      circ_tronco_cm <= quantile(circ_tronco_cm, 0.25) ~ "bajo",
      circ_tronco_cm <= quantile(circ_tronco_cm, 0.50) ~ "medio",
      circ_tronco_cm <= quantile(circ_tronco_cm, 0.75) ~ "alto",
      TRUE ~ "muy alto"
    )
  ) %>% arrange(circ_tronco_cm)
ggplot(result, aes(x = seq(1, nrow(result), by=1), y = circ_tronco_cm, color = circ_tronco_cm_cat)) +
  geom_point() +
  labs(title = "Q Circ Tronco CM",
       x = "Sorted Index",
       y = "CM Circ Tronco") +
  scale_color_manual(values = c("red", "blue", "green", "magenta"))  # Customize colors if needed

write.csv(result, file = "~/workspace/arbolado-mza/arbolado-mza-dataset.csv/arbolado-mza-dataset-circ_tronco_cm-train.csv", row.names = FALSE)

```

# 4.a/b/c/d / 6

```{r}
get_confusion <- function(datay, datay_hat) table(Actual = factor(datay, levels=c(0,1)), Predicted = factor(datay_hat, levels=c(0,1)))
get_stats <- function(confusion){
  TP <- confusion[1, 1]
  TN <- confusion[2, 2]
  FP <- confusion[2, 1]
  FN <- confusion[1, 2]
  
  accuracy <- (TP + TN) / sum(confusion)
  precision <- TP / (TP + FP)
  sensitivity <- TP / (TP + FN)
  specificity <- TN / (TN + FP)
  
  return(data.frame(accuracy=accuracy, precision=precision, sensitivity=sensitivity, specificity=specificity))
}
```

```{r}
set.seed(123)
random_classifier <- function(training_data, p=0.5) function(testing_data) testing_data %>% mutate(y_hat_5050 = ifelse(runif(nrow(testing_data)) > p, 1, 0))
random_predictions <- random_classifier(training_data)(testing_data)
confusion_matrix <- get_confusion(random_predictions$inclinacion_peligrosa, random_predictions$y_hat_5050)
confusion_matrix
get_stats(confusion_matrix)
```

# 5.a/b/c/d / 6




```{r}
biggerclass_classifier <- function(train_data){
  temp <- testing_data %>% group_by(inclinacion_peligrosa) %>% summarize(count=n()) %>% arrange(inclinacion_peligrosa)
  selected_class <- temp[which.max(tab$count), 1]$inclinacion_peligrosa
  return(function (test_data) (test_data %>% mutate(y_hat_bigger = selected_class)))
}
bigger_predictions <- biggerclass_classifier(training_data)(testing_data)
confusion_matrix <- get_confusion(bigger_predictions$inclinacion_peligrosa, bigger_predictions$y_hat_bigger)
confusion_matrix
get_stats(confusion_matrix)
```

# 7
```{r}
tree_classifier <- function(train_data,weights=NULL){
  train_data$inclinacion_peligrosa <- factor(train_data$inclinacion_peligrosa)
  tree_model<-rpart(inclinacion_peligrosa~altura+circ_tronco_cm+lat+long+seccion, data=train_data, weights = weights, minsplit=1, minbucket=1)
  print(tree_model)
  summary(tree_model)
  return(function (test_data){
    temp <- data.frame(test_data)
    temp$y_hat_tree <- predict(tree_model, test_data, type="c")
    return(temp)
  })
}

# Attempt to train the tree with a nearer to 50% split in data, otherwise it just always returns class 0
counts <- training_data %>% group_by(inclinacion_peligrosa) %>% summarize(count=n())
ratio <- (counts[2, 2] / counts[1, 2])[1,1]
```
# Kaggle
```{r}
encode_danger <- function(data, column, out) data %>%
        dplyr::group_by(!!sym(column)) %>%
        dplyr::summarize(count = sum(inclinacion_peligrosa == 1), total = n()) %>%
        dplyr::mutate({{out}} := count / total) %>%
        dplyr::select(-count, -total)
```

```{r}
test_model_output <- function(model, filename){
  root_path <- "~/workspace/arbolado-mza/"
  test.data.set <- read.csv(paste0(root_path, "arbolado-mza-dataset-test.csv/arbolado-mza-dataset-test.csv"))
  
  result.test <- model(testing_data)
  print(result.test)
  confusion_matrix <- get_confusion(result.test$inclinacion_peligrosa, result.test$y_hat_tree)
  print(confusion_matrix)
  print(get_stats(confusion_matrix))
  
  result.validation <- model(test.data.set)
  columns <- data.frame(id = as.numeric(result.validation$id), inclinacion_peligrosa = as.numeric(result.validation$y_hat_tree) - 1)
  write.csv(columns, file = paste0(root_path, "predictions/", filename), row.names=F, append=FALSE, quote=FALSE)
}
```
Con undersampling
```{r}
t <- random_classifier(training_data, ratio)(training_data) %>% filter(inclinacion_peligrosa == 1 | y_hat_5050 == 0)
t %>% group_by(inclinacion_peligrosa) %>% summarize(count=n())
model <- tree_classifier(t)
test_model_output(model, "simple_tree_123.csv")
```

```{r}

weights <- ifelse(training_data$inclinacion_peligrosa == 1, (1-ratio), ratio)
model <- tree_classifier(training_data, weights)
test_model_output(model, "weighted_tree_123.csv")

```

```{r}
rf_classifier <- function(train_data, weights=NULL){
  train_data$inclinacion_peligrosa <- factor(train_data$inclinacion_peligrosa)
  rf_model<-randomForest(inclinacion_peligrosa~altura+circ_tronco_cm+lat+long+seccion, data=train_data, weights = weights, minsplit=1, minbucket=1, ntree=500)
  print(rf_model)
  summary(rf_model)
  return(function (test_data){
    temp <- data.frame(test_data)
    temp$y_hat_tree <- predict(rf_model, test_data, type="c")
    return(temp)
  })
}

weights <- ifelse(training_data$inclinacion_peligrosa == 1, 1*(1-ratio), ratio)
model <- rf_classifier(training_data, weights)
test_model_output(model, "weighted_rf_123.csv")
```

```{r}
# F Engineered RF
rf_fe_classifier <- function(train_data, weights=NULL){
  train_data <- data.frame(train_data)
  especie_a <- encode_danger(train_data, "especie", "especie_a")
  altura_a <- encode_danger(train_data, "altura", "altura_a")
  diametro_tronco_a <- encode_danger(train_data, "diametro_tronco", "diametro_tronco_a")
  nombre_seccion_a <- encode_danger(train_data, "nombre_seccion", "nombre_seccion_a")
  
  train_data <- train_data %>%
    left_join(especie_a, by = "especie") %>%
    left_join(nombre_seccion_a, by = "nombre_seccion") %>%
    left_join(diametro_tronco_a, by = "diametro_tronco") %>%
    left_join(altura_a, by = "altura")
  print(train_data)
  train_data$inclinacion_peligrosa <- factor(train_data$inclinacion_peligrosa)
  rf_model<-randomForest(inclinacion_peligrosa~altura+circ_tronco_cm+lat+long+seccion+especie_a+nombre_seccion_a+diametro_tronco_a+altura_a, data=train_data, weights = weights, minsplit=1, minbucket=1, ntree=1000)
  print(rf_model)
  summary(rf_model)
  return(function (test_data){
    temp <- data.frame(test_data) %>%
      left_join(especie_a, by = "especie") %>%
      left_join(nombre_seccion_a, by = "nombre_seccion") %>%
      left_join(diametro_tronco_a, by = "diametro_tronco") %>%
      left_join(altura_a, by = "altura")
    temp$y_hat_tree <- predict(rf_model, temp, type="c")
    return(temp)
  })
}

t <- random_classifier(training_data, ratio)(training_data) %>% filter(inclinacion_peligrosa == 1 | y_hat_5050 == 0)
t %>% group_by(inclinacion_peligrosa) %>% summarize(count=n())

weights <- ifelse(training_data$inclinacion_peligrosa == 1, 1*(1-ratio), ratio)
weights_t <- ifelse(t$inclinacion_peligrosa == 1, 1*(1-ratio), ratio)

model <- rf_fe_classifier(training_data, weights)
test_model_output(model, "weighted_rf_fe_123.csv")
model <- rf_fe_classifier(t, weights_t)
test_model_output(model, "weighted_rf_fe_123_2.csv")

```

```{r}
rf_cv_classifier <- function(train_data, weights=NULL, folds=10){
  target_column_index <- which(colnames(train_data) == "inclinacion_peligrosa")

  preProc <- preProcess(train_data[, -target_column_index], method = c("center", "scale"))
  training_data_norm <- predict(preProc, train_data[, -target_column_index])
  training_data_norm <- cbind(training_data_norm, inclinacion_peligrosa=train_data$inclinacion_peligrosa)
  print(training_data_norm)
  
  ctrl <- trainControl(method = "cv", number = folds, verboseIter = TRUE)
  set.seed(123)  # for reproducibility
  model <- train(inclinacion_peligrosa~altura+circ_tronco_cm+lat+long+seccion, data = training_data_norm, method = "rf", ntree=50, trControl = ctrl,weights=weights)

  results <- resamples(list(model,model))
  print(model)
  summary(results)
  


  
  # train_data$inclinacion_peligrosa <- factor(train_data$inclinacion_peligrosa)
  # rf_model<-randomForest(inclinacion_peligrosa~altura+circ_tronco_cm+lat+long+seccion, data=train_data, weights = weights, minsplit=1, minbucket=1, ntree=5)
  
  return(function (test_data){
    temp <- data.frame(test_data)
    new_data_norm <- predict(preProc, test_data)
    print(new_data_norm)
    temp$y_hat_tree <- ifelse(predict(model, new_data_norm) > 0.5, 2, 1)
    return(temp)
  })
}



weights <- ifelse(training_data$inclinacion_peligrosa == 1, 1*(1-ratio), ratio)

model <- rf_cv_classifier(training_data, weights, folds=10)
#model <- rf_cv_classifier(t)
test_model_output(model, "weighted_rf_cv_123.csv")
```


```{r}
ggplot(arbolado.mza.dataset, aes(x = lat, y = long, color = especie)) +
  geom_point(aes(alpha=altura), size = 0.3) +
  labs(
    title = "Scatter Plot of Lat vs. Long",
    x = "Latitude",
    y = "Longitude",
    color = "Inclinacion Peligrosa"
  ) +
  #scale_color_gradient(low = "red", high = "green")
  scale_color_manual(values = c(
  "#FF5733", "#337DFF", "#33FF49", "#A833FF", "#FF9B33", "#FF33B6", "#754A23", "#33F4FF",
  "#E133FF", "#FFFA33", "#000000", "#808080", "#AA0000", "#0000AA", "#00AA00", "#AA00AA",
  "#FF5500", "#FF0088", "#5C3317", "#0088FF", "#FF33A1", "#D9B066", "#3E4E50", "#D35400",
  "#E74C3C", "#3498DB", "#229954", "#7D3C98", "#F4D03F", "#34495E", "#F22613", "#2E86C1"
)
)
```


```{r}
# install.packages("plotly")
library(plotly)

# Assuming df is your dataframe with columns lat, long, altura, and especie
plot_ly(data = arbolado.mza.dataset, x = ~lat, y = ~long, z = ~altura, color = ~especie, colors = "Set1") %>%
  add_markers(marker = list(size = 2)) %>%
  layout(
    scene = list(
      xaxis = list(title = "Latitude"),
      yaxis = list(title = "Longitude"),
      zaxis = list(title = "Altura")
    ),
    legend = list(title = "Especie"),
    title = "3D Scatter Plot"
  )
```